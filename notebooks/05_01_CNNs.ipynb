{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f279aef1",
   "metadata": {},
   "source": [
    "# Lecture 5: Supervised learning with CNNs\n",
    "\n",
    "\n",
    "We provide a collab with training and testing code for an MLP and a CNN model classifier. \n",
    "\n",
    "\n",
    "Task 1: Run both examples and discuss the performance difference.\n",
    "\n",
    "\n",
    "### Train a small autotagging model\n",
    "In this section, we will train a small convolutional neural network (CNN) to perform music autotagging using a toy dataset.\n",
    "\n",
    "#### Steps:\n",
    "1. Load and preprocess the dataset\n",
    "2. Define the CNN model\n",
    "3. Train the model\n",
    "4. Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "aa1b6f8c-bec7-4d6c-824c-5ca96bd2af74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: mirdata in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (1.0.0)\n",
      "Requirement already satisfied: chardet>=5.0.0 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from mirdata) (5.2.0)\n",
      "Requirement already satisfied: Deprecated>=1.2.14 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from mirdata) (1.2.18)\n",
      "Requirement already satisfied: h5py>=3.7.0 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from mirdata) (3.14.0)\n",
      "Requirement already satisfied: librosa>=0.10.1 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from mirdata) (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.21.6 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from mirdata) (2.3.3)\n",
      "Requirement already satisfied: pandas>=1.3.5 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from mirdata) (2.3.3)\n",
      "Requirement already satisfied: pretty_midi>=0.2.10 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from mirdata) (0.2.10)\n",
      "Requirement already satisfied: pyyaml>=6.0 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from mirdata) (6.0.3)\n",
      "Requirement already satisfied: openpyxl>=3.0.10 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from mirdata) (3.1.5)\n",
      "Requirement already satisfied: scipy>=1.7.3 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from mirdata) (1.16.2)\n",
      "Requirement already satisfied: tqdm>=4.66.1 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from mirdata) (4.67.1)\n",
      "Requirement already satisfied: smart_open>=5.0.0 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from smart_open[all]>=5.0.0->mirdata) (7.3.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from Deprecated>=1.2.14->mirdata) (1.17.3)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from librosa>=0.10.1->mirdata) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from librosa>=0.10.1->mirdata) (0.62.1)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from librosa>=0.10.1->mirdata) (1.7.2)\n",
      "Requirement already satisfied: joblib>=1.0 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from librosa>=0.10.1->mirdata) (1.5.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from librosa>=0.10.1->mirdata) (5.2.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from librosa>=0.10.1->mirdata) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from librosa>=0.10.1->mirdata) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from librosa>=0.10.1->mirdata) (1.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from librosa>=0.10.1->mirdata) (4.15.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from librosa>=0.10.1->mirdata) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from librosa>=0.10.1->mirdata) (1.1.1)\n",
      "Requirement already satisfied: standard-aifc in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from librosa>=0.10.1->mirdata) (3.13.0)\n",
      "Requirement already satisfied: standard-sunau in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from librosa>=0.10.1->mirdata) (3.13.0)\n",
      "Requirement already satisfied: packaging in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from lazy_loader>=0.1->librosa>=0.10.1->mirdata) (25.0)\n",
      "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from numba>=0.51.0->librosa>=0.10.1->mirdata) (0.45.1)\n",
      "Requirement already satisfied: et-xmlfile in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from openpyxl>=3.0.10->mirdata) (2.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from pandas>=1.3.5->mirdata) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from pandas>=1.3.5->mirdata) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from pandas>=1.3.5->mirdata) (2025.2)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from pooch>=1.1->librosa>=0.10.1->mirdata) (4.4.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from pooch>=1.1->librosa>=0.10.1->mirdata) (2.32.5)\n",
      "Requirement already satisfied: mido>=1.1.16 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from pretty_midi>=0.2.10->mirdata) (1.3.3)\n",
      "Requirement already satisfied: six in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from pretty_midi>=0.2.10->mirdata) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from requests>=2.19.0->pooch>=1.1->librosa>=0.10.1->mirdata) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from requests>=2.19.0->pooch>=1.1->librosa>=0.10.1->mirdata) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from requests>=2.19.0->pooch>=1.1->librosa>=0.10.1->mirdata) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from requests>=2.19.0->pooch>=1.1->librosa>=0.10.1->mirdata) (2025.10.5)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from scikit-learn>=1.1.0->librosa>=0.10.1->mirdata) (3.6.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from soundfile>=0.12.1->librosa>=0.10.1->mirdata) (2.0.0)\n",
      "Requirement already satisfied: pycparser in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa>=0.10.1->mirdata) (2.23)\n",
      "Requirement already satisfied: boto3 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from smart_open>=5.0.0->smart_open[all]>=5.0.0->mirdata) (1.40.46)\n",
      "Requirement already satisfied: google-cloud-storage>=2.6.0 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from smart_open>=5.0.0->smart_open[all]>=5.0.0->mirdata) (3.4.0)\n",
      "Requirement already satisfied: azure-storage-blob in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from smart_open>=5.0.0->smart_open[all]>=5.0.0->mirdata) (12.26.0)\n",
      "Requirement already satisfied: azure-common in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from smart_open>=5.0.0->smart_open[all]>=5.0.0->mirdata) (1.1.28)\n",
      "Requirement already satisfied: azure-core in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from smart_open>=5.0.0->smart_open[all]>=5.0.0->mirdata) (1.35.1)\n",
      "Requirement already satisfied: paramiko in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from smart_open>=5.0.0->smart_open[all]>=5.0.0->mirdata) (4.0.0)\n",
      "Requirement already satisfied: zstandard in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from smart_open>=5.0.0->smart_open[all]>=5.0.0->mirdata) (0.25.0)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.26.1 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from google-cloud-storage>=2.6.0->smart_open>=5.0.0->smart_open[all]>=5.0.0->mirdata) (2.41.1)\n",
      "Requirement already satisfied: google-api-core<3.0.0,>=2.15.0 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from google-cloud-storage>=2.6.0->smart_open>=5.0.0->smart_open[all]>=5.0.0->mirdata) (2.25.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.2 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from google-cloud-storage>=2.6.0->smart_open>=5.0.0->smart_open[all]>=5.0.0->mirdata) (2.4.3)\n",
      "Requirement already satisfied: google-resumable-media<3.0.0,>=2.7.2 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from google-cloud-storage>=2.6.0->smart_open>=5.0.0->smart_open[all]>=5.0.0->mirdata) (2.7.2)\n",
      "Requirement already satisfied: google-crc32c<2.0.0,>=1.1.3 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from google-cloud-storage>=2.6.0->smart_open>=5.0.0->smart_open[all]>=5.0.0->mirdata) (1.7.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from google-api-core<3.0.0,>=2.15.0->google-cloud-storage>=2.6.0->smart_open>=5.0.0->smart_open[all]>=5.0.0->mirdata) (1.70.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from google-api-core<3.0.0,>=2.15.0->google-cloud-storage>=2.6.0->smart_open>=5.0.0->smart_open[all]>=5.0.0->mirdata) (6.32.1)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from google-api-core<3.0.0,>=2.15.0->google-cloud-storage>=2.6.0->smart_open>=5.0.0->smart_open[all]>=5.0.0->mirdata) (1.26.1)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from google-auth<3.0.0,>=2.26.1->google-cloud-storage>=2.6.0->smart_open>=5.0.0->smart_open[all]>=5.0.0->mirdata) (6.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from google-auth<3.0.0,>=2.26.1->google-cloud-storage>=2.6.0->smart_open>=5.0.0->smart_open[all]>=5.0.0->mirdata) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from google-auth<3.0.0,>=2.26.1->google-cloud-storage>=2.6.0->smart_open>=5.0.0->smart_open[all]>=5.0.0->mirdata) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.26.1->google-cloud-storage>=2.6.0->smart_open>=5.0.0->smart_open[all]>=5.0.0->mirdata) (0.6.1)\n",
      "Requirement already satisfied: cryptography>=2.1.4 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from azure-storage-blob->smart_open>=5.0.0->smart_open[all]>=5.0.0->mirdata) (46.0.2)\n",
      "Requirement already satisfied: isodate>=0.6.1 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from azure-storage-blob->smart_open>=5.0.0->smart_open[all]>=5.0.0->mirdata) (0.7.2)\n",
      "Requirement already satisfied: botocore<1.41.0,>=1.40.46 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from boto3->smart_open>=5.0.0->smart_open[all]>=5.0.0->mirdata) (1.40.46)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from boto3->smart_open>=5.0.0->smart_open[all]>=5.0.0->mirdata) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.15.0,>=0.14.0 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from boto3->smart_open>=5.0.0->smart_open[all]>=5.0.0->mirdata) (0.14.0)\n",
      "Requirement already satisfied: bcrypt>=3.2 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from paramiko->smart_open>=5.0.0->smart_open[all]>=5.0.0->mirdata) (5.0.0)\n",
      "Requirement already satisfied: invoke>=2.0 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from paramiko->smart_open>=5.0.0->smart_open[all]>=5.0.0->mirdata) (2.2.0)\n",
      "Requirement already satisfied: pynacl>=1.5 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from paramiko->smart_open>=5.0.0->smart_open[all]>=5.0.0->mirdata) (1.6.0)\n",
      "Requirement already satisfied: standard-chunk in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from standard-aifc->librosa>=0.10.1->mirdata) (3.13.0)\n",
      "Requirement already satisfied: audioop-lts in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from standard-aifc->librosa>=0.10.1->mirdata) (0.2.2)\n",
      "Requirement already satisfied: tqdm in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (4.67.1)\n",
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[15 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "  \u001b[31m   \u001b[0m rather than 'sklearn' for pip commands.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Here is how to fix this error in the main use cases:\n",
      "  \u001b[31m   \u001b[0m - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "  \u001b[31m   \u001b[0m - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "  \u001b[31m   \u001b[0m   (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "  \u001b[31m   \u001b[0m - if the 'sklearn' package is used by one of your dependencies,\n",
      "  \u001b[31m   \u001b[0m   it would be great if you take some time to track which package uses\n",
      "  \u001b[31m   \u001b[0m   'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "  \u001b[31m   \u001b[0m - as a last resort, set the environment variable\n",
      "  \u001b[31m   \u001b[0m   SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m More information is available at\n",
      "  \u001b[31m   \u001b[0m https://github.com/scikit-learn/sklearn-pypi-package\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25hCollecting torchaudio\n",
      "  Downloading torchaudio-2.8.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: torch==2.8.0 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from torchaudio) (2.8.0)\n",
      "Requirement already satisfied: filelock in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from torch==2.8.0->torchaudio) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from torch==2.8.0->torchaudio) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from torch==2.8.0->torchaudio) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from torch==2.8.0->torchaudio) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from torch==2.8.0->torchaudio) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from torch==2.8.0->torchaudio) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from torch==2.8.0->torchaudio) (2025.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch==2.8.0->torchaudio) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages (from jinja2->torch==2.8.0->torchaudio) (3.0.3)\n",
      "Downloading torchaudio-2.8.0-cp313-cp313-macosx_11_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchaudio\n",
      "Successfully installed torchaudio-2.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install mirdata\n",
    "!pip install tqdm\n",
    "!pip install sklearn\n",
    "!pip install torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "62c9a977-516f-4d06-9b3b-d0aa615f9c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import mirdata\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchaudio.transforms import MelSpectrogram, Resample\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6a994b",
   "metadata": {},
   "source": [
    "### Step 1: Load and preprocess the dataset\n",
    "We will use a toy dataset (e.g., ESC-10 or a similar audio dataset). The dataset will be preprocessed using PyTorch's DataLoader utilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5a2aab05-cf7b-4d82-ba24-e63598be79fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Downloading ['audio', 'annotations', 'index']. Index is being stored in /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages/mirdata/datasets/indexes, and the rest of files in /Users/palonso/mir_datasets/tinysol\n",
      "WARNING: [audio] downloading TinySOL.tar.gz\n",
      "WARNING: /Users/palonso/mir_datasets/tinysol/audio/TinySOL.tar.gz already exists and will not be downloaded. Rerun with force_overwrite=True to delete this file and force the download.\n",
      "WARNING: [annotations] downloading TinySOL_metadata.csv\n",
      "WARNING: /Users/palonso/mir_datasets/tinysol/annotation/TinySOL_metadata.csv already exists and will not be downloaded. Rerun with force_overwrite=True to delete this file and force the download.\n",
      "WARNING: [index] downloading tinysol_index_6.0.json\n",
      "WARNING: /Users/palonso/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages/mirdata/datasets/indexes/tinysol_index_6.0.json already exists and will not be downloaded. Rerun with force_overwrite=True to delete this file and force the download.\n"
     ]
    }
   ],
   "source": [
    "dataset = mirdata.initialize(\"tinysol\")\n",
    "dataset.download()\n",
    "splits = dataset.get_random_track_splits([0.8, 0.2])\n",
    "\n",
    "train_ids = splits[0]\n",
    "val_ids = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "4cfec476-f7c1-437b-af46-88fda43f45e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading audio: 100%|█████████████████████████████████████████████████████████████████████████████| 2331/2331 [00:05<00:00, 454.48it/s]\n",
      "Loading audio: 100%|███████████████████████████████████████████████████████████████████████████████| 582/582 [00:01<00:00, 438.50it/s]\n"
     ]
    }
   ],
   "source": [
    "class TinySOLDataset(Dataset):\n",
    "    def __init__(self, mirdata_dataset, ids):\n",
    "\n",
    "\n",
    "        self.orig_sample_rate = 44100\n",
    "        self.sample_rate = 16000\n",
    "\n",
    "        self.audio_duration = 3\n",
    "        \n",
    "        self.mirdata_dataset = mirdata_dataset\n",
    "        self.tids = ids\n",
    "\n",
    "        # Load audio and labels\n",
    "        self.audio = {}\n",
    "        self.natural_labels = {}\n",
    "\n",
    "        self.resample = Resample(orig_freq=self.orig_sample_rate, new_freq=self.sample_rate)\n",
    "\n",
    "        n_samples = self.sample_rate * self.audio_duration\n",
    "        \n",
    "        for tid in tqdm(ids, desc=\"Loading audio\"):\n",
    "            track = self.mirdata_dataset.track(tid)\n",
    "            audio, sr = track.audio\n",
    "\n",
    "            assert sr == self.orig_sample_rate\n",
    "            audio = self.resample(torch.Tensor(audio))\n",
    "\n",
    "            if len(audio) >= n_samples:\n",
    "                audio = audio[:n_samples]\n",
    "            else:\n",
    "                pad_size = n_samples - len(audio)\n",
    "                audio = torch.cat([audio, torch.zeros(pad_size)])\n",
    "            \n",
    "            self.audio[tid] = audio\n",
    "            self.natural_labels[tid] = track.instrument_full\n",
    "\n",
    "        # One hot encode labels\n",
    "        natural_labels = np.array(list(self.natural_labels.values())).reshape(-1, 1)\n",
    "        ohe = OneHotEncoder()\n",
    "        one_hot_labels = ohe.fit_transform(natural_labels).toarray()\n",
    "        self.labels = {k: v for k, v in zip(self.tids, one_hot_labels)}\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tids)\n",
    "\n",
    "    def __getitem__(self, idx, audio_cap=4):\n",
    "        tid = self.tids[idx]\n",
    "        audio = self.audio[tid]\n",
    "        label = self.labels[tid]\n",
    "        \n",
    "        return {\"audio\": audio, \"labels\": label}\n",
    "\n",
    "train_dataset = TinySOLDataset(dataset, train_ids)\n",
    "val_dataset = TinySOLDataset(dataset, val_ids)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128599fd",
   "metadata": {},
   "source": [
    "### Step 2: Define the CNN model\n",
    "We will define a simple CNN architecture suitable for music autotagging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "08422024-5fec-40a9-b53f-469b42a3adba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "d462c091-59c4-4fe3-9339-110d8919192d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extractor. Using mel-spectrogram\n",
    "feature_extractor = MelSpectrogram(n_mels=64, n_fft=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "eb88dc36-793f-4db5-8a76-86a19f331850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mel-spectrogram shape torch.Size([32, 64, 94])\n",
      "Number of classes 14\n"
     ]
    }
   ],
   "source": [
    "n_classes = len(set(val_dataset.natural_labels.values()))\n",
    "\n",
    "batch = next(iter(val_loader))\n",
    "mels = feature_extractor(batch[\"audio\"])\n",
    "\n",
    "print(\"Mel-spectrogram shape:\", mels.shape)\n",
    "print(\"Number of classes:\", n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "afbd18ab-bb3c-4dee-8bbb-8ef89ba6820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "dense_size = 128\n",
    "\n",
    "mlp = MLP( 64 * 94, dense_size, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "16cd4e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        kernel_size: int=3,\n",
    "        poolin_size: int=4,\n",
    "        dense_size: int=64,\n",
    "        n_classes: int=10,\n",
    "    ):\n",
    "        super(CNN, self).__init__()\n",
    "        self.dense_size = dense_size\n",
    "        self.conv1 = nn.Conv2d(1, dense_size // 4, kernel_size=kernel_size, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(dense_size // 4, dense_size// 2, kernel_size=kernel_size, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(dense_size // 2, dense_size, kernel_size=kernel_size, stride=1, padding=1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size=poolin_size, stride=poolin_size)\n",
    "        \n",
    "        self.fc1 = nn.Linear(dense_size, dense_size)\n",
    "        self.fc2 = nn.Linear(dense_size, n_classes)  # Assuming 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        \n",
    "        x = x.view(-1, self.dense_size)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "cnn = CNN(dense_size=dense_size, n_classes=n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23555686",
   "metadata": {},
   "source": [
    "### Step 3: Train the models\n",
    "Set up the training loop with loss functions and optimizers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "f6ab91d7-39e1-4c41-a3f4-b483128da519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 64, 94])"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "c8d62092-5f5c-4bf6-b7a4-c27d88a38081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 76.0935\n",
      "Epoch [2/20], Loss: 40.3076\n",
      "Epoch [3/20], Loss: 13.5963\n",
      "Epoch [4/20], Loss: 21.6461\n",
      "Epoch [5/20], Loss: 13.2569\n",
      "Epoch [6/20], Loss: 3.4883\n",
      "Epoch [7/20], Loss: 10.2740\n",
      "Epoch [8/20], Loss: 4.9114\n",
      "Epoch [9/20], Loss: 4.2719\n",
      "Epoch [10/20], Loss: 9.1220\n",
      "Epoch [11/20], Loss: 21.3596\n",
      "Epoch [12/20], Loss: 25.6680\n",
      "Epoch [13/20], Loss: 4.2586\n",
      "Epoch [14/20], Loss: 62.4288\n",
      "Epoch [15/20], Loss: 4.8110\n",
      "Epoch [16/20], Loss: 26.1160\n",
      "Epoch [17/20], Loss: 13.6851\n",
      "Epoch [18/20], Loss: 44.1044\n",
      "Epoch [19/20], Loss: 17.6623\n",
      "Epoch [20/20], Loss: 17.7802\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "n_epochs = 3\n",
    "for epoch in range(n_epochs):  # Number of epochs\n",
    "    model.train()\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        mels = feature_extractor(batch[\"audio\"])\n",
    "        # Just flatten the input\n",
    "        mels = mels.reshape(-1, 64 * 94)\n",
    "        outputs = mlp(mels).softmax(dim=1)\n",
    "\n",
    "        loss = criterion(outputs, batch[\"labels\"])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{n_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "867d6833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 2.6619\n",
      "Epoch [2/10], Loss: 2.6313\n",
      "Epoch [3/10], Loss: 2.6439\n",
      "Epoch [4/10], Loss: 2.6755\n",
      "Epoch [5/10], Loss: 2.6316\n",
      "Epoch [6/10], Loss: 2.6087\n",
      "Epoch [7/10], Loss: 2.6670\n",
      "Epoch [8/10], Loss: 2.6086\n",
      "Epoch [9/10], Loss: 2.6661\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[310]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m     outputs = cnn(mels).softmax(dim=\u001b[32m1\u001b[39m)\n\u001b[32m     15\u001b[39m     loss = criterion(outputs, batch[\u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     optimizer.step()\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss.item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/reps/machine_learning_for_sound_and_music/venv/lib/python3.13/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "n_epochs = 3\n",
    "for epoch in range(n_epochs):  # Number of epochs\n",
    "    model.train()\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        mels = feature_extractor(batch[\"audio\"])\n",
    "        outputs = cnn(mels).softmax(dim=1)\n",
    "\n",
    "        loss = criterion(outputs, batch[\"labels\"])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{n_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "8e1fe3d4-b3d0-49cb-ac3e-a041e5e48c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 14])"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "978aeefa-9943-406f-b378-8f5624930a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2048 / 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc88d66-0b39-456c-a5ca-11f916ed37dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlfsam",
   "language": "python",
   "name": "mlfsam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
