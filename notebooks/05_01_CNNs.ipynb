{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f279aef1",
   "metadata": {},
   "source": [
    "# Lecture 5: Supervised learning with CNNs\n",
    "\n",
    "\n",
    "We provide a collab with training and testing code for an MLP and a CNN model classifier. \n",
    "\n",
    "\n",
    "Task 1: Run both examples and discuss the performance difference.\n",
    "\n",
    "\n",
    "### Train a small autotagging model\n",
    "In this section, we will train a small convolutional neural network (CNN) to perform music autotagging using a toy dataset.\n",
    "\n",
    "#### Steps:\n",
    "1. Load and preprocess the dataset\n",
    "2. Define the CNN model\n",
    "3. Train the model\n",
    "4. Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1b6f8c-bec7-4d6c-824c-5ca96bd2af74",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install mirdata\n",
    "!pip install tqdm\n",
    "!pip install sklearn\n",
    "!pip install torchaudio\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c9a977-516f-4d06-9b3b-d0aa615f9c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mirdata\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchaudio.transforms import MelSpectrogram, Resample\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6a994b",
   "metadata": {},
   "source": [
    "### Step 1: Load and preprocess the dataset\n",
    "We will use a toy dataset (e.g., ESC-10 or a similar audio dataset). The dataset will be preprocessed using PyTorch's DataLoader utilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2aab05-cf7b-4d82-ba24-e63598be79fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = mirdata.initialize(\"tinysol\")\n",
    "dataset.download()\n",
    "split = dataset.get_random_track_splits([0.8, 0.2], split_names=(\"train\", \"val\"))\n",
    "train_ids, val_ids = split[\"train\"], split[\"val\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfec476-f7c1-437b-af46-88fda43f45e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinySOLDataset(Dataset):\n",
    "    def __init__(self, mirdata_dataset, ids):\n",
    "        self.orig_sample_rate = 44100\n",
    "        self.sample_rate = 16000\n",
    "\n",
    "        self.audio_duration = 3\n",
    "        \n",
    "        self.mirdata_dataset = mirdata_dataset\n",
    "        self.tids = ids\n",
    "\n",
    "        # Load audio and labels\n",
    "        self.audio = {}\n",
    "        self.natural_labels = {}\n",
    "\n",
    "        self.resample = Resample(orig_freq=self.orig_sample_rate, new_freq=self.sample_rate)\n",
    "\n",
    "        n_samples = self.sample_rate * self.audio_duration\n",
    "        \n",
    "        for tid in tqdm(ids, desc=\"Loading audio\"):\n",
    "            track = self.mirdata_dataset.track(tid)\n",
    "            audio, sr = track.audio\n",
    "\n",
    "            assert sr == self.orig_sample_rate\n",
    "            audio = self.resample(torch.Tensor(audio))\n",
    "\n",
    "            if len(audio) >= n_samples:\n",
    "                audio = audio[:n_samples]\n",
    "            else:\n",
    "                pad_size = n_samples - len(audio)\n",
    "                audio = torch.cat([audio, torch.zeros(pad_size)])\n",
    "            \n",
    "            self.audio[tid] = audio\n",
    "            self.natural_labels[tid] = track.instrument_full\n",
    "\n",
    "        # One hot encode labels\n",
    "        natural_labels = np.array(list(self.natural_labels.values())).reshape(-1, 1)\n",
    "        ohe = OneHotEncoder()\n",
    "        one_hot_labels = ohe.fit_transform(natural_labels).toarray()\n",
    "        self.labels = {k: v for k, v in zip(self.tids, one_hot_labels)}\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tids)\n",
    "\n",
    "    def __getitem__(self, idx, audio_cap=4):\n",
    "        # TODO audio_cap not used.\n",
    "        tid = self.tids[idx]\n",
    "        audio = self.audio[tid]\n",
    "        labels = self.labels[tid]\n",
    "        natural_labels = self.natural_labels[tid]\n",
    "        \n",
    "        return {\"audio\": audio, \"labels\": labels, \"natural_labels\": natural_labels}\n",
    "\n",
    "train_dataset = TinySOLDataset(dataset, train_ids)\n",
    "val_dataset = TinySOLDataset(dataset, val_ids)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128599fd",
   "metadata": {},
   "source": [
    "### Step 2: Define the CNN model\n",
    "We will define a simple CNN architecture suitable for music autotagging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d462c091-59c4-4fe3-9339-110d8919192d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extractor. Using mel-spectrogram\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, randomize=False, **kwargs):\n",
    "        super().__init__()\n",
    "        self.randomize = randomize\n",
    "        \n",
    "        self.melspectrogram = MelSpectrogram(**kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # # Apply permutation\n",
    "        # if self.randomize:\n",
    "        #     x = x[:, torch.randperm(x.size(1))]\n",
    "\n",
    "        x = self.melspectrogram(x)\n",
    "        x = torch.log10(1 + 1000 * x)  # Log-compression\n",
    "\n",
    "        # # Apply permutation\n",
    "        if self.randomize:\n",
    "            x_s = x.shape\n",
    "            x = torch.flatten(x, 1, 2)\n",
    "            x = x[:, torch.randperm(x.size(1))]\n",
    "            x = x.view(x_s)\n",
    "        return x\n",
    "\n",
    "feature_extractor = FeatureExtractor(n_mels=64, n_fft=1024, randomize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ee667a-eb55-4fe9-8095-8caa8d5c25fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot mel-spectrograms for debugging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "feature_extractor = FeatureExtractor(n_mels=64, n_fft=1024)\n",
    "\n",
    "# Fetch a batch of data\n",
    "batch = next(iter(train_loader))\n",
    "mel_specs = feature_extractor(batch[\"audio\"]).detach().numpy()\n",
    "# Visualize the first three mel-spectrograms in the batch\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for i in range(3):\n",
    "    axes[i].imshow(mel_specs[i], aspect=\"auto\", origin=\"lower\")\n",
    "    axes[i].set_title(f\"Example {i+1}: {batch['natural_labels'][i]}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb88dc36-793f-4db5-8a76-86a19f331850",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(set(val_dataset.natural_labels.values()))\n",
    "\n",
    "batch = next(iter(val_loader))\n",
    "mels = feature_extractor(batch[\"audio\"])\n",
    "\n",
    "print(\"Mel-spectrogram shape:\", mels.shape)\n",
    "print(\"Number of classes:\", n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbd18ab-bb3c-4dee-8bbb-8ef89ba6820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        dense_size: int=64,\n",
    "        n_classes: int=10,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, dense_size)\n",
    "        self.fc2 = nn.Linear(dense_size, dense_size)\n",
    "        self.fc3 = nn.Linear(dense_size, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1, 2)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cd4e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        kernel_size: int=3,\n",
    "        poolin_size: int=4,\n",
    "        dense_size: int=64,\n",
    "        n_classes: int=10,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dense_size = dense_size\n",
    "        self.conv1 = nn.Conv2d(1, dense_size // 4, kernel_size=kernel_size, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(dense_size // 4, dense_size// 2, kernel_size=kernel_size, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(dense_size // 2, dense_size, kernel_size=kernel_size, stride=1, padding=1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size=poolin_size, stride=poolin_size)\n",
    "        self.fc1 = nn.Linear(dense_size, dense_size)\n",
    "        self.fc2 = nn.Linear(dense_size, n_classes)  # Assuming 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        \n",
    "        x = x.view(-1, self.dense_size)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23555686",
   "metadata": {},
   "source": [
    "### Step 3: Train the models\n",
    "Set up the training loop with loss functions and optimizers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d62092-5f5c-4bf6-b7a4-c27d88a38081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train(model, n_epochs=3, lr=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # Training loop\n",
    "        model.train()\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            x, y = batch[\"audio\"], batch[\"labels\"]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Extract features and forward step\n",
    "            x = feature_extractor(x)\n",
    "            y_est = model(x)\n",
    "\n",
    "            loss = criterion(y_est, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        loss_train = loss.item()\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        accs = []\n",
    "        for i, batch in enumerate(val_loader):\n",
    "            x, y = batch[\"audio\"], batch[\"labels\"]\n",
    "            \n",
    "            # Extract features and forward model\n",
    "            x = feature_extractor(x)\n",
    "            y_est = model(x)\n",
    "            \n",
    "            loss = criterion(y_est, y)\n",
    "\n",
    "            y_est = y_est.softmax(dim=1)\n",
    "            y_int = torch.argmax(y, dim=1).numpy()\n",
    "            y_est_int = torch.argmax(y_est, dim=1).numpy()\n",
    "            accs.append(accuracy_score(y_int, y_est_int))\n",
    "        \n",
    "        acc_val = np.mean(accs)\n",
    "        loss_val = loss.item()\n",
    "\n",
    "        print(\n",
    "            f'Epoch [{epoch + 1}/{n_epochs}], ' +\n",
    "            f'Train loss: {loss_train:.4f}, ' +\n",
    "            f'Val loss: {loss_val:.4f}, ' +\n",
    "            f'Val acc.: {acc_val:.4f}'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef5200b-8ac5-41b8-a46b-b8e866dc72bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(mels.shape[1] * mels.shape[2], dense_size=32, n_classes=n_classes)\n",
    "num_params = sum(p.numel() for p in mlp.parameters() if p.requires_grad)\n",
    "print(f\"MLP parameters: {num_params}\")\n",
    "\n",
    "cnn = CNN(dense_size=128, n_classes=n_classes)\n",
    "num_params = sum(p.numel() for p in cnn.parameters() if p.requires_grad)\n",
    "print(f\"CNN Trainable parameters: {num_params}\")\n",
    "\n",
    "\n",
    "print(\"\\nTraining MLP model\")\n",
    "train(mlp, n_epochs=10)\n",
    "\n",
    "print(\"\\nTraining CNN model\")\n",
    "train(cnn, n_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113cac7b-7624-4447-b6ad-36d93d19e98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = FeatureExtractor(n_mels=64, n_fft=1024, randomize=True)\n",
    "\n",
    "# Fetch a batch of data\n",
    "batch = next(iter(train_loader))\n",
    "mel_specs = feature_extractor(batch[\"audio\"]).detach().numpy()\n",
    "\n",
    "# Visualize the first three mel-spectrograms in the batch\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for i in range(3):\n",
    "    axes[i].imshow(mel_specs[i], aspect='auto', origin='lower')\n",
    "    axes[i].set_title(f\"Example {i+1}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ce6fae-69b5-471c-8cfe-1d48ec101155",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(mels.shape[1] * mels.shape[2], dense_size=32, n_classes=n_classes)\n",
    "num_params = sum(p.numel() for p in mlp.parameters() if p.requires_grad)\n",
    "print(f\"MLP parameters: {num_params}\")\n",
    "\n",
    "cnn = CNN(dense_size=128, n_classes=n_classes)\n",
    "num_params = sum(p.numel() for p in cnn.parameters() if p.requires_grad)\n",
    "print(f\"CNN Trainable parameters: {num_params}\")\n",
    "\n",
    "\n",
    "print(\"\\nTraining MLP model\")\n",
    "train(mlp, n_epochs=10)\n",
    "\n",
    "print(\"\\nTraining CNN model\")\n",
    "train(cnn, n_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c782e4cc-a2fa-438c-b57d-267387c9c667",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
