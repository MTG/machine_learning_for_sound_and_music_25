{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f279aef1",
      "metadata": {
        "id": "f279aef1"
      },
      "source": [
        "# Lecture 7: Metric learning\n",
        "\n",
        "\n",
        "We provide a colab with training, inference, and clustering code using CNNs trained with metric learning approaches.\n",
        "\n",
        "## Notebook steps:\n",
        "1. Train a CNN with the triplet loss\n",
        "2. Plot the clustering results of the validation set\n",
        "3. Train a CNN with the InfoNCE loss\n",
        "4. Perform clustering of the validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa1b6f8c-bec7-4d6c-824c-5ca96bd2af74",
      "metadata": {
        "id": "aa1b6f8c-bec7-4d6c-824c-5ca96bd2af74"
      },
      "outputs": [],
      "source": [
        "!pip install torch\n",
        "!pip install torchcodec\n",
        "!pip install datasets[audio]==3\n",
        "!pip install tqdm\n",
        "!pip install scikit-learn\n",
        "!pip install matplotlib\n",
        "!pip install torch-audiomentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62c9a977-516f-4d06-9b3b-d0aa615f9c03",
      "metadata": {
        "id": "62c9a977-516f-4d06-9b3b-d0aa615f9c03"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import umap\n",
        "from datasets import load_dataset\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchaudio.transforms import MelSpectrogram, Resample\n",
        "from torch_audiomentations import (\n",
        "    Compose,\n",
        "    Gain,\n",
        "    PolarityInversion,\n",
        "    AddBackgroundNoise,\n",
        "    AddColoredNoise,\n",
        "    BandStopFilter,\n",
        "    ApplyImpulseResponse,\n",
        "    PitchShift,\n",
        "    LowPassFilter,\n",
        ")\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use Datasets to load GTZAN\n",
        "ds = load_dataset(\"sanchit-gandhi/gtzan\", split=\"train\")\n",
        "\n",
        "# make random train/val splits\n",
        "splits = ds.train_test_split(test_size=0.2, seed=42)\n",
        "train_ds = splits[\"train\"]\n",
        "val_ds = splits[\"test\"]"
      ],
      "metadata": {
        "id": "ufd1mMMEMAE6"
      },
      "id": "ufd1mMMEMAE6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cfec476-f7c1-437b-af46-88fda43f45e4",
      "metadata": {
        "id": "4cfec476-f7c1-437b-af46-88fda43f45e4"
      },
      "outputs": [],
      "source": [
        "class GTZANDataset(Dataset):\n",
        "    \"Multiview GTZAN Genre dataset based on HuggingFace Datasets\"\n",
        "    def __init__(self, dataset, device):\n",
        "        self.orig_sample_rate = 32000\n",
        "        self.sample_rate = 16000\n",
        "        self.audio_duration = 3  # Segment durations\n",
        "        self.max_separation = 1  # maximum separation between views\n",
        "        self.device =  device\n",
        "\n",
        "        self.dataset = dataset\n",
        "\n",
        "        self.n_samples = self.sample_rate * self.audio_duration\n",
        "\n",
        "        self.resample = Resample(orig_freq=self.orig_sample_rate, new_freq=self.sample_rate)\n",
        "\n",
        "        self.keep_genres = set([\"rock\", \"classical\", \"jazz\", \"disco\"])\n",
        "\n",
        "        # Load audio and labels\n",
        "        self.audio = {}\n",
        "        self.natural_labels = {}\n",
        "        for track in tqdm(self.dataset, desc=\"Loading audio\"):\n",
        "            tid = Path(track[\"file\"]).stem\n",
        "            genre = tid.split(\".\")[0]\n",
        "\n",
        "            if genre not in self.keep_genres:\n",
        "                continue\n",
        "\n",
        "            x = track[\"audio\"][\"array\"]\n",
        "            x = torch.tensor(x)\n",
        "            sr = track[\"audio\"][\"sampling_rate\"]\n",
        "\n",
        "            if sr != self.sample_rate:\n",
        "              if sr != self.orig_sample_rate:\n",
        "                self.resample = Resample(orig_freq=sr, new_freq=self.sample_rate)\n",
        "                self.orig_sample_rate = sr\n",
        "\n",
        "              x = self.resample(x.float())\n",
        "\n",
        "            self.audio[tid] = x.to(self.device)\n",
        "            self.natural_labels[tid] = genre\n",
        "\n",
        "        self.tids = list(self.audio.keys())\n",
        "\n",
        "        # One hot encode labels\n",
        "        natural_labels = np.array(list(self.natural_labels.values())).reshape(-1, 1)\n",
        "        ohe = OneHotEncoder()\n",
        "        one_hot_labels = ohe.fit_transform(natural_labels).toarray()\n",
        "        self.labels = {k: v for k, v in zip(self.tids, one_hot_labels)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tids)\n",
        "\n",
        "    def get_start_end_indices(self, min, max):\n",
        "        start = random.randint(min, max - self.n_samples)\n",
        "        end = start + self.n_samples\n",
        "        return start, end\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tid = self.tids[idx]\n",
        "        audio_full = self.audio[tid]\n",
        "        labels = self.labels[tid]\n",
        "        natural_labels = self.natural_labels[tid]\n",
        "\n",
        "        # get the first view from a random position in the track.\n",
        "        min_pos, max_pos = 0, audio_full.shape[0]\n",
        "\n",
        "        start, end = self.get_start_end_indices(min_pos, max_pos)\n",
        "        view_1 = audio_full[start:end]\n",
        "\n",
        "        # the second view should start within a window of n seconds from  the first view.\n",
        "        min_pos = max(0, min_pos - self.max_separation * self.sample_rate)\n",
        "        max_pos = min(audio_full.shape[0], max_pos + self.max_separation * self.sample_rate)\n",
        "\n",
        "        start, end = self.get_start_end_indices(min_pos, max_pos)\n",
        "        view_2 = audio_full[start:end]\n",
        "\n",
        "\n",
        "        return {\"view_1\": view_1, \"view_2\": view_2, \"labels\": labels, \"natural_labels\": natural_labels}\n",
        "\n",
        "device = \"cuda\"\n",
        "train_dataset = GTZANDataset(train_ds, device=device)\n",
        "val_dataset = GTZANDataset(val_ds, device=device)\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "128599fd",
      "metadata": {
        "id": "128599fd"
      },
      "source": [
        "### Step 2: Define the CNN model\n",
        "We will define a simple CNN architecture suitable for music autotagging.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9Er39IPe40vZ"
      },
      "id": "9Er39IPe40vZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d462c091-59c4-4fe3-9339-110d8919192d",
      "metadata": {
        "id": "d462c091-59c4-4fe3-9339-110d8919192d"
      },
      "outputs": [],
      "source": [
        "# Feature extractor. Using mel-spectrogram\n",
        "\n",
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "        self.melspectrogram = MelSpectrogram(**kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.melspectrogram(x)\n",
        "        x = torch.log10(1 + 1000 * x)  # Log-compression\n",
        "        return x\n",
        "\n",
        "class AugmentationChain(nn.Module):\n",
        "    \"\"\"Add a number of audio augmentaitons with probability p.\"\"\"\n",
        "\n",
        "    p = 0.75\n",
        "    sr = 16000\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "        self.chain = Compose(\n",
        "            transforms=[\n",
        "                Gain(p=self.p, output_type='tensor'),\n",
        "                BandStopFilter(sample_rate=self.sr, p=self.p, output_type='tensor'),\n",
        "                PitchShift(p=self.p, sample_rate=self.sr, output_type='tensor'),\n",
        "                AddColoredNoise(p=self.p, output_type='tensor'),\n",
        "                LowPassFilter(p=self.p, sample_rate=self.sr, min_cutoff_freq=400, output_type='tensor',),\n",
        "            ]\n",
        "            , output_type='tensor'\n",
        "        )\n",
        "\n",
        "    def forward(self, audio):\n",
        "        return self.chain(audio.unsqueeze(1)).squeeze(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14ee667a-eb55-4fe9-8095-8caa8d5c25fa",
      "metadata": {
        "scrolled": true,
        "id": "14ee667a-eb55-4fe9-8095-8caa8d5c25fa"
      },
      "outputs": [],
      "source": [
        "feature_extractor = FeatureExtractor(n_mels=64, n_fft=1024).to(device)\n",
        "augmentation_chain = AugmentationChain().to(device)\n",
        "\n",
        "# Fetch a batch of data\n",
        "batch = next(iter(val_loader))\n",
        "mel_specs_1 = feature_extractor(augmentation_chain(batch[\"view_1\"])).cpu().numpy()\n",
        "mel_specs_2 = feature_extractor(augmentation_chain(batch[\"view_2\"])).cpu().detach().numpy()\n",
        "\n",
        "# Visualize the first three mel-spectrograms in the batch\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "for i, mels in enumerate([mel_specs_1, mel_specs_2]):\n",
        "    axes[i].imshow(mels[0], aspect=\"auto\", origin=\"lower\")\n",
        "    axes[i].set_title(f\"Example {0+1}: {batch['natural_labels'][0]}, view ({i})\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb88dc36-793f-4db5-8a76-86a19f331850",
      "metadata": {
        "id": "eb88dc36-793f-4db5-8a76-86a19f331850"
      },
      "outputs": [],
      "source": [
        "n_classes = len(set(val_dataset.natural_labels.values()))\n",
        "\n",
        "batch = next(iter(val_loader))\n",
        "mels = feature_extractor(batch[\"view_1\"])\n",
        "\n",
        "print(\"Mel-spectrogram shape:\", mels.shape)\n",
        "print(\"Number of classes:\", n_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16cd4e60",
      "metadata": {
        "id": "16cd4e60"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        kernel_size: int = 3,\n",
        "        pooling_size: int = 4,\n",
        "        dense_size: int = 64,\n",
        "        output_size: int = 10,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dense_size = dense_size\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            1, dense_size // 4, kernel_size=kernel_size, stride=1, padding=1\n",
        "        )\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            dense_size // 4, dense_size // 2, kernel_size=kernel_size, stride=1, padding=1\n",
        "        )\n",
        "        self.conv3 = nn.Conv2d(\n",
        "            dense_size // 2, dense_size, kernel_size=kernel_size, stride=1, padding=1\n",
        "        )\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(dense_size // 4)\n",
        "        self.bn2 = nn.BatchNorm2d(dense_size // 2)\n",
        "        self.bn3 = nn.BatchNorm2d(dense_size)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=pooling_size, stride=pooling_size)\n",
        "        self.fc1 = nn.Linear(dense_size, dense_size)\n",
        "        self.fc2 = nn.Linear(dense_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        x = self.bn1(self.pool(F.relu(self.conv1(x))))\n",
        "        x = self.bn2(self.pool(F.relu(self.conv2(x))))\n",
        "        x = self.bn3(self.pool(F.relu(self.conv3(x))))\n",
        "\n",
        "        x = x.view(-1, self.dense_size)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23555686",
      "metadata": {
        "id": "23555686"
      },
      "source": [
        "### Step 3: Train the models\n",
        "Set up the training loop with loss functions and optimizers.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "def train(\n",
        "    model,\n",
        "    loss_fn,\n",
        "    n_epochs=3,\n",
        "    lr=0.001,\n",
        "    device=\"cuda\",\n",
        "    augment=True,\n",
        "    batch_size=64,\n",
        "):\n",
        "    # Use Adam optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        # Training loop\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for i, batch in enumerate(train_loader):\n",
        "            x1 = batch[\"view_1\"].to(device)\n",
        "            x2 = batch[\"view_2\"].to(device)\n",
        "\n",
        "            if augment:\n",
        "              x1 = augmentation_chain(x1)\n",
        "              x2 = augmentation_chain(x2)\n",
        "\n",
        "            x1 = feature_extractor(x1)\n",
        "            x2 = feature_extractor(x2)\n",
        "\n",
        "            # Forward step for both augmentations\n",
        "            z1 = model(x1)\n",
        "            z2 = model(x2)\n",
        "            loss = loss_fn(z1, z2)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Backprop\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        train_loss = total_loss / len(train_loader)\n",
        "\n",
        "        # Validation loop (still using cross-entropy for evaluation)\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "\n",
        "        for i, batch in enumerate(val_loader):\n",
        "            x1 = batch[\"view_1\"].to(device)\n",
        "            x2 = batch[\"view_2\"].to(device)\n",
        "\n",
        "            if augment:\n",
        "                x1 = augmentation_chain(x1)\n",
        "                x2 = augmentation_chain(x2)\n",
        "\n",
        "            x1 = feature_extractor(x1)\n",
        "            x2 = feature_extractor(x2)\n",
        "\n",
        "            # Forward step for both augmentations\n",
        "            z1 = model(x1)\n",
        "            z2 = model(x2)\n",
        "            loss = loss_fn(z1, z2)\n",
        "\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "        val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "        print(\n",
        "            f\"Epoch [{epoch + 1}/{n_epochs}], \"\n",
        "            + f\"Train loss: {train_loss:.4f}, \"\n",
        "            + f\"Val loss: {val_loss:.4f}\"\n",
        "        )"
      ],
      "metadata": {
        "id": "NqmFYaZjDvZL"
      },
      "id": "NqmFYaZjDvZL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def triplet_loss(anchor, positive, margin=1.0):\n",
        "    # Pick a random negative from the batch (next sample)\n",
        "    negative = torch.roll(positive, 1, dims=0)\n",
        "\n",
        "    distance_positive = (anchor - positive).pow(2).sum(1)\n",
        "    distance_negative = (anchor - negative).pow(2).sum(1)\n",
        "\n",
        "    losses = F.relu(distance_positive - distance_negative + margin)\n",
        "    return losses.mean()"
      ],
      "metadata": {
        "id": "_h8cuRcOD1A4"
      },
      "id": "_h8cuRcOD1A4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8d62092-5f5c-4bf6-b7a4-c27d88a38081",
      "metadata": {
        "id": "c8d62092-5f5c-4bf6-b7a4-c27d88a38081"
      },
      "outputs": [],
      "source": [
        "def InfoNCE(z_1, z_2, temp=0.5):\n",
        "    # Normalize the embeddings\n",
        "    z_1 = F.normalize(z_1, dim=1)\n",
        "    z_2 = F.normalize(z_2, dim=1)\n",
        "\n",
        "    # Similarity matrices\n",
        "    sim_12 = torch.matmul(z_1, z_2.T)  # z1 as anchor, z2 as candidates\n",
        "    sim_21 = torch.matmul(z_2, z_1.T)  # z2 as anchor, z1 as candidates\n",
        "\n",
        "    # Labels: each sample i should match its own pair i\n",
        "    labels = torch.arange(sim_12.size(0)).to(sim_12.device)\n",
        "\n",
        "    # InfoNCE losses (both directions)\n",
        "    loss_12 = F.cross_entropy(sim_12 / temp, labels)\n",
        "    loss_21 = F.cross_entropy(sim_21 / temp, labels)\n",
        "\n",
        "    # Symmetric SimCLR loss\n",
        "    return 0.5 * (loss_12 + loss_21)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract embeddings for the validation set\n",
        "def plot_val_embeddings(model, device=\"cuda\"):\n",
        "    model.eval()\n",
        "    embeddings = []\n",
        "    natural_labels = []\n",
        "\n",
        "    model.to(device)\n",
        "    feature_extractor.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for key, audio in val_dataset.audio.items():\n",
        "            n_samples = val_dataset.n_samples\n",
        "            cut = audio.shape[0] % n_samples\n",
        "            audio = audio[: -cut]\n",
        "            batch = audio.view(-1, n_samples)\n",
        "            x = feature_extractor(audio).unsqueeze(0)\n",
        "            z = model(x)\n",
        "            z = torch.mean(z, dim=0)\n",
        "\n",
        "            embeddings.append(z.cpu().numpy())\n",
        "            natural_labels.append(val_dataset.natural_labels[key])\n",
        "\n",
        "    embeddings = np.stack(embeddings)\n",
        "    natural_labels = np.array(natural_labels)\n",
        "\n",
        "    # Perform UMAP\n",
        "    reducer = umap.UMAP(n_components=2)\n",
        "    umap_embeddings = reducer.fit_transform(embeddings)\n",
        "\n",
        "    # Create UMAP plot\n",
        "    unique_labels, label_indices = np.unique(natural_labels, return_inverse=True)\n",
        "\n",
        "    cmap = matplotlib.cm.get_cmap(\"tab10\", len(unique_labels))\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    scatter = plt.scatter(\n",
        "        umap_embeddings[:, 0],\n",
        "        umap_embeddings[:, 1],\n",
        "        c=label_indices,\n",
        "        cmap=cmap,\n",
        "    )\n",
        "\n",
        "    handles, _ = scatter.legend_elements(prop=\"colors\", num=len(unique_labels))\n",
        "    plt.legend(handles, unique_labels, title=\"Labels\")\n",
        "\n",
        "    plt.title(\"UMAP of Audio Embeddings\")\n",
        "    plt.xlabel(\"UMAP 1\")\n",
        "    plt.ylabel(\"UMAP 2\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "P1MLT2p97YkT"
      },
      "id": "P1MLT2p97YkT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = CNN(dense_size=64, output_size=64).to(device)\n",
        "\n",
        "num_params = sum(p.numel() for p in cnn.parameters() if p.requires_grad)\n",
        "print(f\"CNN Trainable parameters: {num_params}\")\n",
        "\n",
        "plot_val_embeddings(cnn)"
      ],
      "metadata": {
        "id": "YLjeFbUDgvHN"
      },
      "id": "YLjeFbUDgvHN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = CNN(dense_size=64, output_size=64).to(device)\n",
        "\n",
        "print(\"\\nTraining model with the triplet loss\")\n",
        "train(cnn, triplet_loss, n_epochs=20, device=device, batch_size=64)\n",
        "plot_val_embeddings(cnn)"
      ],
      "metadata": {
        "id": "oRVNQqijBcVh"
      },
      "id": "oRVNQqijBcVh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eef5200b-8ac5-41b8-a46b-b8e866dc72bc",
      "metadata": {
        "id": "eef5200b-8ac5-41b8-a46b-b8e866dc72bc"
      },
      "outputs": [],
      "source": [
        "cnn = CNN(dense_size=64, output_size=64).to(device)\n",
        "\n",
        "print(\"\\nTraining model with the InfoNCE loss\")\n",
        "train(cnn, InfoNCE, n_epochs=20, device=device, batch_size=64)\n",
        "plot_val_embeddings(cnn)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}